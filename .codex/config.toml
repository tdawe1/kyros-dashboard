# Kyros Dashboard – Codex CLI configuration (project-local)

# Default model/provider: balanced code + reasoning
model = "gpt-5"
model_provider = "openai"

# Default execution posture: productive, still safe
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# Clickable file links target VS Code (adjust if you use something else)
file_opener = "cursor"
preferred_auth_method = "chatgpt"

[sandbox_workspace_write]
# Stay offline by default inside the sandbox; prevents accidental network calls
network_access = false
exclude_tmpdir_env_var = false
exclude_slash_tmp = false
writable_roots = []

[shell_environment_policy]
# Pass a minimal, predictable environment to subprocesses to avoid leaking secrets
inherit = "core"
ignore_default_excludes = false
exclude = []
set = {}
include_only = ["HOME", "PATH", "USER", "SHELL"]

[history]
persistence = "save-all"

# Trust this working tree for smoother edits (Git metadata remains protected by sandbox)
[projects."/home/thomas/kyros-dashboard"]
trust_level = "trusted"

# --- Profiles ---
# Switch with: codex --profile safe|dev|deep
profile = "gpt5_high"

[profiles.safe]
approval_policy = "untrusted"
sandbox_mode = "read-only"

[profiles.dev]
approval_policy = "on-request"
sandbox_mode = "workspace-write"

[profiles.deep]
model = "o3"
model_provider = "openai"
approval_policy = "on-request"
sandbox_mode = "workspace-write"
model_reasoning_effort = "high"
model_reasoning_summary = "concise"

[profiles.gpt5_high]
model = "gpt-5"
model_provider = "openai"
approval_policy = "on-request"
sandbox_mode = "workspace-write"
model_reasoning_effort = "high"
model_reasoning_summary = "concise"

# Two implementer profiles (OpenAI) to run in parallel if desired
[profiles.impl_a]
model = "gpt-5"
model_provider = "openai"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

[profiles.impl_b]
model = "gpt-5"
model_provider = "openai"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# Reviewer with stricter posture
[profiles.review_strict]
model = "gpt-5"
model_provider = "openai"
approval_policy = "untrusted"
sandbox_mode = "read-only"
model_reasoning_summary = "concise"

# --- MCP servers available in this repo ---
# These start from the repo root and auto-load .env via mcp/env.py
[mcp_servers.kyros_collab]
command = "python3"
args = ["-m", "mcp.kyros_collab_server"]

[mcp_servers.linear]
command = "python3"
args = ["-m", "mcp.linear_server"]

[mcp_servers.railway]
command = "python3"
args = ["-m", "mcp.railway_server"]

[mcp_servers.vercel]
command = "python3"
args = ["-m", "mcp.vercel_server"]

[mcp_servers.coderabbit]
command = "python3"
args = ["-m", "mcp.coderabbit_server"]

# Optional switches you can uncomment as needed:
# disable_response_storage = true  # For ZDR orgs
# model_supports_reasoning_summaries = true
# model_verbosity = "low"         # For terse GPT‑5 outputs
# [tools]
# web_search = true

# --- Non-OpenAI model providers via OpenAI-compatible proxies ---
# Adjust base_url/ports to your gateway setup. These proxies should accept
# OpenAI Chat Completions semantics and forward to Anthropic/Google.
[model_providers.claude_proxy]
name = "Anthropic via Proxy"
base_url = "http://localhost:4000/v1"
env_key = "ANTHROPIC_API_KEY"
wire_api = "chat"

[model_providers.gemini_proxy]
name = "Gemini via Proxy"
base_url = "http://localhost:4001/v1"
env_key = "GOOGLE_API_KEY"
wire_api = "chat"

# --- Role-based profiles using your requested models ---

# Architect/Planner: careful decomposition, reviews, design docs
[profiles.architect_sonnet]
model = "claude-4.1-sonnet"
model_provider = "claude_proxy"
approval_policy = "on-request"
sandbox_mode = "workspace-write"
model_reasoning_effort = "high"
model_reasoning_summary = "concise"

# Implementer: mainline coding with larger context and solid API synthesis
[profiles.impl_gemini_pro]
model = "gemini-2.5-pro"
model_provider = "gemini_proxy"
approval_policy = "on-request"
sandbox_mode = "workspace-write"

# Speed coder: scaffolding, small refactors, quick iterations
[profiles.speed_gemini_flash]
model = "gemini-2.5-flash"
model_provider = "gemini_proxy"
approval_policy = "on-request"
sandbox_mode = "workspace-write"
